{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea6ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed93549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MemberName</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>Attendance</th>\n",
       "      <th>TotalHours</th>\n",
       "      <th>AssignmentsCompleted</th>\n",
       "      <th>HackathonParticipation</th>\n",
       "      <th>GitHubScore</th>\n",
       "      <th>PeerReviewScore</th>\n",
       "      <th>CourseName</th>\n",
       "      <th>CapstoneScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Theekshana Rathnayake</td>\n",
       "      <td>3</td>\n",
       "      <td>79.9</td>\n",
       "      <td>43.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>62.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Foundations of ML</td>\n",
       "      <td>45.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mayura Sandakalum Sellapperuma</td>\n",
       "      <td>2</td>\n",
       "      <td>76.8</td>\n",
       "      <td>95.6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>87.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>Foundations of ML</td>\n",
       "      <td>78.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amila Narangoda</td>\n",
       "      <td>3</td>\n",
       "      <td>96.6</td>\n",
       "      <td>75.9</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>98.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>Foundations of ML</td>\n",
       "      <td>65.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tharusha Vihanga</td>\n",
       "      <td>2</td>\n",
       "      <td>83.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>41.8</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Foundations of ML</td>\n",
       "      <td>40.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chamath Perera</td>\n",
       "      <td>3</td>\n",
       "      <td>86.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Foundations of ML</td>\n",
       "      <td>68.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       MemberName  EducationLevel  Attendance  TotalHours  \\\n",
       "0           Theekshana Rathnayake               3        79.9        43.7   \n",
       "1  Mayura Sandakalum Sellapperuma               2        76.8        95.6   \n",
       "2                 Amila Narangoda               3        96.6        75.9   \n",
       "4                Tharusha Vihanga               2        83.2        24.0   \n",
       "7                  Chamath Perera               3        86.5        88.0   \n",
       "\n",
       "   AssignmentsCompleted  HackathonParticipation  GitHubScore  PeerReviewScore  \\\n",
       "0                     2                       0         62.8              5.0   \n",
       "1                     6                       0         87.4              2.7   \n",
       "2                     8                       0         98.4              2.8   \n",
       "4                     6                       0         41.8              4.2   \n",
       "7                     5                       0         23.9              1.3   \n",
       "\n",
       "          CourseName  CapstoneScore  \n",
       "0  Foundations of ML           45.3  \n",
       "1  Foundations of ML           78.8  \n",
       "2  Foundations of ML           65.4  \n",
       "4  Foundations of ML           40.1  \n",
       "7  Foundations of ML           68.2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('zuu crew scores.csv')\n",
    "df = df[df['CourseName'] == 'Foundations of ML']\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7799a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    \"Attendance\",\n",
    "    \"TotalHours\",\n",
    "    \"AssignmentsCompleted\",\n",
    "    \"HackathonParticipation\",\n",
    "    \"PeerReviewScore\",  # swap to \"GitHubScore\" if you prefer\n",
    "]\n",
    "TARGET = \"CapstoneScore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79e97811",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = df[FEATURES].values.astype(float)\n",
    "y = df[TARGET].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a34b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_base = X_raw.mean(axis=0)\n",
    "sigma_base = X_raw.std(axis=0, ddof=0)\n",
    "sigma_base_safe = np.where(sigma_base == 0, 1.0, sigma_base)\n",
    "Z = (X_raw - mu_base) / sigma_base_safe     # (m,5)  <-- this is what was missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, beta):\n",
    "    m = len(y)\n",
    "    y_hat = X.dot(beta)\n",
    "    return (1/(2*m)) * np.sum((y_hat - y)**2)\n",
    "\n",
    "def gradient_descent(X, y, beta, lr, n_iter):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    for i in range(n_iter):\n",
    "        y_hat = X.dot(beta)\n",
    "        gradients = (1/m) * X.T.dot(y_hat - y)\n",
    "        beta = beta - lr * gradients\n",
    "        cost = compute_cost(X, y, beta)\n",
    "        cost_history.append(cost)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Cost {cost:.4f}\")\n",
    "    return cost_history, beta\n",
    "\n",
    "def predict(X, beta):\n",
    "    return X.dot(beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc71db8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Polynomial degree 1 (lr=0.01) ===\n",
      "Iteration : 0: Cost : 1628.3791\n",
      "Iteration : 100: Cost : 227.5618\n",
      "Iteration : 200: Cost : 40.3059\n",
      "Iteration : 300: Cost : 14.9248\n",
      "Iteration : 400: Cost : 11.4194\n",
      "Iteration : 500: Cost : 10.9214\n",
      "Iteration : 600: Cost : 10.8476\n",
      "Iteration : 700: Cost : 10.8360\n",
      "Iteration : 800: Cost : 10.8340\n",
      "Iteration : 900: Cost : 10.8337\n",
      "Iteration : 1000: Cost : 10.8336\n",
      "Iteration : 1100: Cost : 10.8336\n",
      "Iteration : 1200: Cost : 10.8336\n",
      "Iteration : 1300: Cost : 10.8336\n",
      "Iteration : 1400: Cost : 10.8336\n",
      "Iteration : 1500: Cost : 10.8336\n",
      "Iteration : 1600: Cost : 10.8336\n",
      "Iteration : 1700: Cost : 10.8336\n",
      "Iteration : 1800: Cost : 10.8336\n",
      "Iteration : 1900: Cost : 10.8336\n",
      "Iteration : 2000: Cost : 10.8336\n",
      "Iteration : 2100: Cost : 10.8336\n",
      "Iteration : 2200: Cost : 10.8336\n",
      "Iteration : 2300: Cost : 10.8336\n",
      "Iteration : 2400: Cost : 10.8336\n",
      "Iteration : 2500: Cost : 10.8336\n",
      "Iteration : 2600: Cost : 10.8336\n",
      "Iteration : 2700: Cost : 10.8336\n",
      "Iteration : 2800: Cost : 10.8336\n",
      "Iteration : 2900: Cost : 10.8336\n",
      "Iteration : 3000: Cost : 10.8336\n",
      "Iteration : 3100: Cost : 10.8336\n",
      "Iteration : 3200: Cost : 10.8336\n",
      "Iteration : 3300: Cost : 10.8336\n",
      "Iteration : 3400: Cost : 10.8336\n",
      "Iteration : 3500: Cost : 10.8336\n",
      "Iteration : 3600: Cost : 10.8336\n",
      "Iteration : 3700: Cost : 10.8336\n",
      "Iteration : 3800: Cost : 10.8336\n",
      "Iteration : 3900: Cost : 10.8336\n",
      "Iteration : 4000: Cost : 10.8336\n",
      "Iteration : 4100: Cost : 10.8336\n",
      "Iteration : 4200: Cost : 10.8336\n",
      "Iteration : 4300: Cost : 10.8336\n",
      "Iteration : 4400: Cost : 10.8336\n",
      "Iteration : 4500: Cost : 10.8336\n",
      "Iteration : 4600: Cost : 10.8336\n",
      "Iteration : 4700: Cost : 10.8336\n",
      "Iteration : 4800: Cost : 10.8336\n",
      "Iteration : 4900: Cost : 10.8336\n",
      "Iteration : 5000: Cost : 10.8336\n",
      "Iteration : 5100: Cost : 10.8336\n",
      "Iteration : 5200: Cost : 10.8336\n",
      "Iteration : 5300: Cost : 10.8336\n",
      "Iteration : 5400: Cost : 10.8336\n",
      "Iteration : 5500: Cost : 10.8336\n",
      "Iteration : 5600: Cost : 10.8336\n",
      "Iteration : 5700: Cost : 10.8336\n",
      "Iteration : 5800: Cost : 10.8336\n",
      "Iteration : 5900: Cost : 10.8336\n",
      "Iteration : 6000: Cost : 10.8336\n",
      "Iteration : 6100: Cost : 10.8336\n",
      "Iteration : 6200: Cost : 10.8336\n",
      "Iteration : 6300: Cost : 10.8336\n",
      "Iteration : 6400: Cost : 10.8336\n",
      "Iteration : 6500: Cost : 10.8336\n",
      "Iteration : 6600: Cost : 10.8336\n",
      "Iteration : 6700: Cost : 10.8336\n",
      "Iteration : 6800: Cost : 10.8336\n",
      "Iteration : 6900: Cost : 10.8336\n",
      "\n",
      "=== Training Polynomial degree 2 (lr=0.01) ===\n",
      "Iteration : 0: Cost : 1627.6125\n",
      "Iteration : 100: Cost : 222.6507\n",
      "Iteration : 200: Cost : 40.0224\n",
      "Iteration : 300: Cost : 14.8833\n",
      "Iteration : 400: Cost : 10.9535\n",
      "Iteration : 500: Cost : 10.0826\n",
      "Iteration : 600: Cost : 9.7514\n",
      "Iteration : 700: Cost : 9.5700\n",
      "Iteration : 800: Cost : 9.4565\n",
      "Iteration : 900: Cost : 9.3825\n",
      "Iteration : 1000: Cost : 9.3334\n",
      "Iteration : 1100: Cost : 9.3006\n",
      "Iteration : 1200: Cost : 9.2785\n",
      "Iteration : 1300: Cost : 9.2637\n",
      "Iteration : 1400: Cost : 9.2536\n",
      "Iteration : 1500: Cost : 9.2468\n",
      "Iteration : 1600: Cost : 9.2422\n",
      "Iteration : 1700: Cost : 9.2391\n",
      "Iteration : 1800: Cost : 9.2370\n",
      "Iteration : 1900: Cost : 9.2355\n",
      "Iteration : 2000: Cost : 9.2346\n",
      "Iteration : 2100: Cost : 9.2339\n",
      "Iteration : 2200: Cost : 9.2335\n",
      "Iteration : 2300: Cost : 9.2331\n",
      "Iteration : 2400: Cost : 9.2329\n",
      "Iteration : 2500: Cost : 9.2328\n",
      "Iteration : 2600: Cost : 9.2327\n",
      "Iteration : 2700: Cost : 9.2326\n",
      "Iteration : 2800: Cost : 9.2326\n",
      "Iteration : 2900: Cost : 9.2326\n",
      "Iteration : 3000: Cost : 9.2325\n",
      "Iteration : 3100: Cost : 9.2325\n",
      "Iteration : 3200: Cost : 9.2325\n",
      "Iteration : 3300: Cost : 9.2325\n",
      "Iteration : 3400: Cost : 9.2325\n",
      "Iteration : 3500: Cost : 9.2325\n",
      "Iteration : 3600: Cost : 9.2325\n",
      "Iteration : 3700: Cost : 9.2325\n",
      "Iteration : 3800: Cost : 9.2325\n",
      "Iteration : 3900: Cost : 9.2325\n",
      "Iteration : 4000: Cost : 9.2325\n",
      "Iteration : 4100: Cost : 9.2325\n",
      "Iteration : 4200: Cost : 9.2325\n",
      "Iteration : 4300: Cost : 9.2325\n",
      "Iteration : 4400: Cost : 9.2325\n",
      "Iteration : 4500: Cost : 9.2325\n",
      "Iteration : 4600: Cost : 9.2325\n",
      "Iteration : 4700: Cost : 9.2325\n",
      "Iteration : 4800: Cost : 9.2325\n",
      "Iteration : 4900: Cost : 9.2325\n",
      "Iteration : 5000: Cost : 9.2325\n",
      "Iteration : 5100: Cost : 9.2325\n",
      "Iteration : 5200: Cost : 9.2325\n",
      "Iteration : 5300: Cost : 9.2325\n",
      "Iteration : 5400: Cost : 9.2325\n",
      "Iteration : 5500: Cost : 9.2325\n",
      "Iteration : 5600: Cost : 9.2325\n",
      "Iteration : 5700: Cost : 9.2325\n",
      "Iteration : 5800: Cost : 9.2325\n",
      "Iteration : 5900: Cost : 9.2325\n",
      "Iteration : 6000: Cost : 9.2325\n",
      "Iteration : 6100: Cost : 9.2325\n",
      "Iteration : 6200: Cost : 9.2325\n",
      "Iteration : 6300: Cost : 9.2325\n",
      "Iteration : 6400: Cost : 9.2325\n",
      "Iteration : 6500: Cost : 9.2325\n",
      "Iteration : 6600: Cost : 9.2325\n",
      "Iteration : 6700: Cost : 9.2325\n",
      "Iteration : 6800: Cost : 9.2325\n",
      "Iteration : 6900: Cost : 9.2325\n",
      "\n",
      "=== Training Polynomial degree 3 (lr=0.003) ===\n",
      "Iteration : 0: Cost : 1648.1959\n",
      "Iteration : 100: Cost : 848.9471\n",
      "Iteration : 200: Cost : 468.0402\n",
      "Iteration : 300: Cost : 261.2697\n",
      "Iteration : 400: Cost : 147.8124\n",
      "Iteration : 500: Cost : 85.4191\n",
      "Iteration : 600: Cost : 51.0345\n",
      "Iteration : 700: Cost : 32.0259\n",
      "Iteration : 800: Cost : 21.4653\n",
      "Iteration : 900: Cost : 15.5519\n",
      "Iteration : 1000: Cost : 12.1995\n",
      "Iteration : 1100: Cost : 10.2622\n",
      "Iteration : 1200: Cost : 9.1102\n",
      "Iteration : 1300: Cost : 8.3968\n",
      "Iteration : 1400: Cost : 7.9309\n",
      "Iteration : 1500: Cost : 7.6068\n",
      "Iteration : 1600: Cost : 7.3658\n",
      "Iteration : 1700: Cost : 7.1750\n",
      "Iteration : 1800: Cost : 7.0160\n",
      "Iteration : 1900: Cost : 6.8782\n",
      "Iteration : 2000: Cost : 6.7554\n",
      "Iteration : 2100: Cost : 6.6439\n",
      "Iteration : 2200: Cost : 6.5412\n",
      "Iteration : 2300: Cost : 6.4459\n",
      "Iteration : 2400: Cost : 6.3570\n",
      "Iteration : 2500: Cost : 6.2736\n",
      "Iteration : 2600: Cost : 6.1951\n",
      "Iteration : 2700: Cost : 6.1211\n",
      "Iteration : 2800: Cost : 6.0512\n",
      "Iteration : 2900: Cost : 5.9849\n",
      "Iteration : 3000: Cost : 5.9221\n",
      "Iteration : 3100: Cost : 5.8625\n",
      "Iteration : 3200: Cost : 5.8058\n",
      "Iteration : 3300: Cost : 5.7518\n",
      "Iteration : 3400: Cost : 5.7004\n",
      "Iteration : 3500: Cost : 5.6513\n",
      "Iteration : 3600: Cost : 5.6044\n",
      "Iteration : 3700: Cost : 5.5596\n",
      "Iteration : 3800: Cost : 5.5167\n",
      "Iteration : 3900: Cost : 5.4757\n",
      "Iteration : 4000: Cost : 5.4363\n",
      "Iteration : 4100: Cost : 5.3986\n",
      "Iteration : 4200: Cost : 5.3623\n",
      "Iteration : 4300: Cost : 5.3275\n",
      "Iteration : 4400: Cost : 5.2941\n",
      "Iteration : 4500: Cost : 5.2619\n",
      "Iteration : 4600: Cost : 5.2310\n",
      "Iteration : 4700: Cost : 5.2011\n",
      "Iteration : 4800: Cost : 5.1724\n",
      "Iteration : 4900: Cost : 5.1447\n",
      "Iteration : 5000: Cost : 5.1180\n",
      "Iteration : 5100: Cost : 5.0923\n",
      "Iteration : 5200: Cost : 5.0674\n",
      "Iteration : 5300: Cost : 5.0433\n",
      "Iteration : 5400: Cost : 5.0201\n",
      "Iteration : 5500: Cost : 4.9977\n",
      "Iteration : 5600: Cost : 4.9760\n",
      "Iteration : 5700: Cost : 4.9550\n",
      "Iteration : 5800: Cost : 4.9346\n",
      "Iteration : 5900: Cost : 4.9149\n",
      "Iteration : 6000: Cost : 4.8959\n",
      "Iteration : 6100: Cost : 4.8774\n",
      "Iteration : 6200: Cost : 4.8595\n",
      "Iteration : 6300: Cost : 4.8421\n",
      "Iteration : 6400: Cost : 4.8253\n",
      "Iteration : 6500: Cost : 4.8090\n",
      "Iteration : 6600: Cost : 4.7931\n",
      "Iteration : 6700: Cost : 4.7777\n",
      "Iteration : 6800: Cost : 4.7628\n",
      "Iteration : 6900: Cost : 4.7483\n",
      "\n",
      "=== Model comparison ===\n",
      "Degree 1: R²=0.9242, RMSE=4.6548, MSE=21.6671, Features=6\n",
      "Degree 2: R²=0.9354, RMSE=4.2971, MSE=18.4650, Features=21\n",
      "Degree 3: R²=0.9669, RMSE=3.0771, MSE=9.4686, Features=56\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "def polynomial_features_matrix(Z, degree):\n",
    "    m, n = Z.shape\n",
    "    cols = [np.ones((m, 1))]  # bias\n",
    "    for deg in range(1, degree+1):\n",
    "        for comb in combinations_with_replacement(range(n), deg):\n",
    "            prod = np.prod(Z[:, comb], axis=1, keepdims=True)\n",
    "            cols.append(prod)\n",
    "    return np.hstack(cols)\n",
    "\n",
    "def train_poly_full_dataset(degree, lr=0.01, n_iter=6000, rescale_poly=True):\n",
    "    Xp = polynomial_features_matrix(Z, degree)\n",
    "\n",
    "    if rescale_poly:\n",
    "        Xp_nb = Xp[:, 1:]  # exclude bias\n",
    "        mu_p = Xp_nb.mean(axis=0)\n",
    "        sigma_p = Xp_nb.std(axis=0, ddof=0)\n",
    "        sigma_p_safe = np.where(sigma_p == 0, 1.0, sigma_p)\n",
    "        Xp_scaled = np.c_[np.ones((Xp.shape[0], 1)), (Xp_nb - mu_p)/sigma_p_safe]\n",
    "    else:\n",
    "        Xp_scaled = Xp\n",
    "\n",
    "    beta0 = np.zeros(Xp_scaled.shape[1])\n",
    "    cost_history, beta = gradient_descent(Xp_scaled, y, beta0, lr, n_iter)\n",
    "    y_pred = predict(Xp_scaled, beta)\n",
    "\n",
    "    def mse(y_true, y_pred): return np.mean((y_true - y_pred)**2)\n",
    "    def rmse(y_true, y_pred): return np.sqrt(mse(y_true, y_pred))\n",
    "    def r2(y_true, y_pred):\n",
    "        ss_res = np.sum((y_true - y_pred)**2)\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "        return 1 - ss_res/ss_tot\n",
    "\n",
    "    return {\n",
    "        \"degree\": degree,\n",
    "        \"beta\": beta,\n",
    "        \"cost_history\": cost_history,\n",
    "        \"mse\": mse(y, y_pred),\n",
    "        \"rmse\": rmse(y, y_pred),\n",
    "        \"r2\": r2(y, y_pred),\n",
    "        \"y_pred\": y_pred,\n",
    "        \"n_features\": Xp_scaled.shape[1]\n",
    "    }\n",
    "\n",
    "# ---------- Run degrees 1, 2, 3 ----------\n",
    "results = {}\n",
    "for deg, alpha in [(1, 0.01), (2, 0.01), (3, 0.003)]:\n",
    "    print(f\"\\n=== Training Polynomial degree {deg} (lr={alpha}) ===\")\n",
    "    results[deg] = train_poly_full_dataset(degree=deg, lr=alpha, n_iter=7000)\n",
    "\n",
    "# ---------- Summarize ----------\n",
    "print(\"\\n=== Model comparison ===\")\n",
    "for d in [1,2,3]:\n",
    "    r = results[d]\n",
    "    print(f\"Degree {d}: R²={r['r2']:.4f}, RMSE={r['rmse']:.4f}, \"\n",
    "          f\"MSE={r['mse']:.4f}, Features={r['n_features']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
